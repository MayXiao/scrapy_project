{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# path = r\"D:\\merged_v1(2)\\merged_v1\"\n",
    "# os.chdir(path)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "import time\n",
    "import urllib.parse\n",
    "\n",
    "import datetime\n",
    "import io\n",
    "# from io import BytesIO\n",
    "from tika import parser\n",
    "import PyPDF2\n",
    "from tqdm import tqdm  \n",
    "import random\n",
    "import string\n",
    "import tldextract\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "import scrapy.crawler as crawler\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from pdf_downloader.spiders.pdf_spider import PDFSpider\n",
    "# from scrapy.crawler import CrawlerRunner\n",
    "from twisted.internet import reactor,defer\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from scrapy.exceptions import CloseSpider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read company file and get related webpages using Google API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read company info file\n",
    "df = pd.read_excel('Result_HERO_Companies w NZ Targets (06.06.23).xlsx',sheet_name = 'NZT DATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain(url):\n",
    "    \"\"\"\n",
    "    get company site domain\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.netloc\n",
    "    if domain.startswith('www.'):\n",
    "        domain = domain[4:]  # Remove 'www.' if present\n",
    "\n",
    "    if \"/\" not in domain:\n",
    "        # If \".com\" is not in the url, return the original url\n",
    "        return domain\n",
    "    split_domain = domain.split(\"/\", 1)\n",
    "\n",
    "    result = split_domain[0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(columns=['result_1', 'result_2', 'result_3'])\n",
    "\n",
    "api_endpoint = 'https://api.scaleserp.com/search'   \n",
    "api_key = ''\n",
    "\n",
    "def search_and_filter_results(terms, company_web):\n",
    "    \"\"\"\n",
    "    Using Google API query and top3 into dateframe\n",
    "    \"\"\"\n",
    "    url = f\"{api_endpoint}?api_key={api_key}\"\n",
    "    # query = f\"{terms} site:{company_web} filetype: pdf \"\n",
    "    query = f\"{terms} site:{company_web}\"\n",
    "    params = {\n",
    "        'q': query,\n",
    "        # 'hl': 'en',\n",
    "        'num': '3'\n",
    "    }\n",
    "    print(query)\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if 'organic_results' in data:\n",
    "            results = data['organic_results']\n",
    "            print([result for result in results][:3])\n",
    "            domain = tldextract.extract(company_web).domain\n",
    "            filtered_results = [result for result in results if tldextract.extract(result['domain']).domain in domain]\n",
    "            return filtered_results[:3]\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    terms = '(sustainability OR climate OR ESG) report 2022 pdf'\n",
    "    company_web = row['website']\n",
    "    domain = get_domain(row['website'])\n",
    "    filtered_results = search_and_filter_results(terms, domain)\n",
    "    print(filtered_results)\n",
    "    if filtered_results:\n",
    "        result_urls = [result['link'] for result in filtered_results]\n",
    "        df_results.loc[index, 'id_code'] = row['id_code']\n",
    "        df_results.loc[index, 'result_1'] = result_urls[0] if len(result_urls) > 0 else np.nan\n",
    "        df_results.loc[index, 'result_2'] = result_urls[1] if len(result_urls) > 1 else np.nan\n",
    "        df_results.loc[index, 'result_3'] = result_urls[2] if len(result_urls) > 2 else np.nan\n",
    "        df_results.loc[index, 'search_query'] = terms + \" \" + \"site:\" + domain\n",
    "        df_results.loc[index, 'company_mainsite'] = company_web\n",
    "        df_results.loc[index, 'domain'] = domain\n",
    "        df_results.loc[index, 'actor_name'] = df.loc[index, 'name']\n",
    "\n",
    "df['domain']= df.website.apply(get_domain)\n",
    "sub_df = df[['id_code','website','name','domain']]\n",
    "new_column_names = ['id_code','company_mainsite','actor_name','domain']\n",
    "sub_df = sub_df.rename(columns=dict(zip(sub_df.columns, new_column_names)))\n",
    "df_results = pd.merge(sub_df,df_results, how='left', on = ['id_code','company_mainsite','actor_name','domain']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check whether API returned urls contains 2023 reports and store urls in a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agents list \n",
    "user_agents = [\n",
    "    # Google Chrome\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36',\n",
    "\n",
    "    # Mozilla Firefox\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:96.0) Gecko/20100101 Firefox/96.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:96.0) Gecko/20100101 Firefox/96.0',\n",
    "\n",
    "    # Apple Safari\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.61 Safari/537.36 Edg/94.0.992.31',\n",
    "\n",
    "    # Microsoft Edge\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36 Edg/96.0.1054.29',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36 Edg/96.0.1054.29',\n",
    "\n",
    "    # Opera\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36 OPR/63.0.3368.71',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36 OPR/63.0.3368.71',\n",
    "\n",
    "    # More Google Chrome\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36',\n",
    "\n",
    "    # More Mozilla Firefox\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:97.0) Gecko/20100101 Firefox/97.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:97.0) Gecko/20100101 Firefox/97.0',\n",
    "\n",
    "    # More Apple Safari\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.3 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36 Edg/95.0.1020.30',\n",
    "\n",
    "    # More Microsoft Edge\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36 Edg/97.0.1072.62',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36 Edg/97.0.1072.62',\n",
    "\n",
    "    # More Opera\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36 OPR/64.0.3417.83',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36 OPR/64.0.3417.83',\n",
    "]\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': random.choice(user_agents)\n",
    "}\n",
    "\n",
    "additional_user_agents = [\n",
    "    # Google Chrome\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4700.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_16) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4700.0 Safari/537.36',\n",
    "\n",
    "    # Mozilla Firefox\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_16; rv:98.0) Gecko/20100101 Firefox/98.0',\n",
    "\n",
    "    # Apple Safari\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_16) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4700.0 Safari/537.36 Edg/98.0.1075.0',\n",
    "\n",
    "    # Microsoft Edge\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4700.0 Safari/537.36 Edg/98.0.1075.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_16) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4700.0 Safari/537.36 Edg/98.0.1075.0',\n",
    "\n",
    "    # Opera\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4700.0 Safari/537.36 OPR/65.0.3467.48',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_16) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4700.0 Safari/537.36 OPR/65.0.3467.48',\n",
    "]\n",
    "\n",
    "additional_headers = {\n",
    "    'User-Agent': random.choice(additional_user_agents)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pdf_url(url):\n",
    "    \"\"\"\n",
    "    Check url is PDF\n",
    "    \"\"\"\n",
    "    if urllib.parse.urlparse(url).port == 443:\n",
    "        time.sleep(2)  # adjust the sleep time as needed\n",
    "    response = requests.head(url)\n",
    "    content_type = response.headers.get('content-type')\n",
    "    if content_type and content_type.lower() == 'application/pdf':\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_punctuation_and_newlines(text):\n",
    "    \"\"\"\n",
    "    remove punctuation and newlines in the text\n",
    "    \"\"\"\n",
    "    chinese_punctuation = \"，。！？【】（）％＃＠＆１２３４５６７８９０：；“”‘’《》\"\n",
    "    translator = str.maketrans('', '', string.punctuation + chinese_punctuation)\n",
    "    stripped_text = text.translate(translator)\n",
    "    stripped_text = stripped_text.replace('\\n', '')\n",
    "    return stripped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pdf_file_for_keyword(url, year):\n",
    "    \"\"\"\n",
    "    read pdf url to check whether pdf is created on 2023 and contains keywords\n",
    "    \"\"\"\n",
    "\n",
    "    # add a sleep time if port is 443\n",
    "    if urllib.parse.urlparse(url).port == 443:\n",
    "        time.sleep(random.uniform(1.0, 5.0)) # adjust the sleep time as needed\n",
    "    response = requests.get(url, verify=False)\n",
    "    parsed = parser.from_buffer(response.content)\n",
    "    text = parsed['content']\n",
    "    create_date = None\n",
    "    text_short = strip_punctuation_and_newlines(text)[:300]\n",
    "    try:\n",
    "        pdf_content = io.BytesIO(response.content)\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_content)\n",
    "        metadata = pdf_reader.metadata\n",
    "        create_date = metadata.get('/CreationDate')\n",
    "        create_date = int(create_date[2:6])\n",
    "        print(create_date)\n",
    "    except:\n",
    "        if '2022' in text_short:\n",
    "            create_date = 2023\n",
    "        elif '2021' in text_short:\n",
    "            create_date = 2022\n",
    "        elif '2020' in text_short:\n",
    "            create_date = 2021\n",
    "            print(create_date)\n",
    "    if len([ i for i in patterns + search_terms if i in text_short.lower()])>0 \\\n",
    "        and (create_date in year)\\\n",
    "        and ('report' in text_short.lower()):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_out_less2page(url):\n",
    "    \"\"\"\n",
    "    filter out pdf less than 4 pages\n",
    "    \"\"\"\n",
    "    # add a sleep time if port is 443\n",
    "    if urllib.parse.urlparse(url).port == 443:\n",
    "        time.sleep(random.uniform(3.0, 7.0)) # adjust the sleep time as needed\n",
    "    response = requests.get(url, verify=False, headers=headers, timeout=60)\n",
    "    try:\n",
    "        pdf_content = io.BytesIO(response.content)\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_content)\n",
    "        page_count = len(pdf_reader.pages)\n",
    "    except:\n",
    "        page_count = 4\n",
    "    if page_count > 3:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_create_date_page_num(pdf_url):\n",
    "    \"\"\"\n",
    "    get pdf create date and page number\n",
    "    \"\"\"\n",
    "    if urllib.parse.urlparse(pdf_url).port == 443:\n",
    "        time.sleep(random.uniform(2.0, 7.0))  # adjust the sleep time as needed\n",
    "    try:\n",
    "        response = requests.get(pdf_url,verify=False, headers=additional_headers, timeout=60)\n",
    "        pdf_content = io.BytesIO(response.content)\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_content)\n",
    "        metadata = pdf_reader.metadata\n",
    "        create_date = metadata.get('/CreationDate')\n",
    "        create_date = int(create_date[2:6])\n",
    "        page_count = len(pdf_reader.pages)\n",
    "        return create_date, int(page_count) if page_count else 0\n",
    "    except:\n",
    "        try: \n",
    "            if urllib.parse.urlparse(pdf_url).port == 443:\n",
    "                time.sleep(random.uniform(4.0, 6.0))  # adjust the sleep time as needed\n",
    "            response = requests.get(pdf_url, verify=False, headers=headers, timeout=60)\n",
    "            parsed = parser.from_buffer(response.content)\n",
    "            text = parsed['content']\n",
    "            pdf_content = io.BytesIO(response.content)\n",
    "            pdf_reader = PyPDF2.PdfReader(pdf_content)\n",
    "            page_count = len(pdf_reader.pages)\n",
    "            text_short = strip_punctuation_and_newlines(text)[:300]\n",
    "            if '2022' in text_short:\n",
    "                return 2023, int(page_count) if page_count else 0\n",
    "            elif '2021' in text_short:\n",
    "                return 2022, int(page_count) if page_count else 0\n",
    "            elif '2020' in text_short:\n",
    "                return 2021, int(page_count) if page_count else 0          \n",
    "            elif '2019' in text_short:\n",
    "                return 2020, int(page_count) if page_count else 0           \n",
    "        except:\n",
    "            return 2022,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_url(row, dictionary):\n",
    "    \"\"\"\n",
    "    Find the latest year pdf url with the longest pages\n",
    "    \"\"\"\n",
    "    allow_domain = row\n",
    "    # allowed_domain = get_domain(row[\"website\"])\n",
    "    nested_dict = dictionary.get(allow_domain, {})\n",
    "    max_first_value = float('-inf')\n",
    "    max_second_value = float('-inf')\n",
    "    max_url = None\n",
    "\n",
    "    for url, (first_value, second_value) in nested_dict.items():\n",
    "        if first_value > max_first_value:\n",
    "            max_first_value = first_value\n",
    "            max_url = url\n",
    "            # print(max_first_url)\n",
    "\n",
    "        if first_value == max_first_value and second_value > max_second_value:\n",
    "            max_second_value = second_value\n",
    "            max_url = url\n",
    "\n",
    "\n",
    "    return max_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"pdf url keywords filter\"\n",
    "patterns = [\n",
    "    \"sustain\",\n",
    "    \"esg\",\n",
    "    \"environment\",\n",
    "    \"climate\",\n",
    "    \"green\",\n",
    "    \"responsibility\",\n",
    "    \"csr\",\n",
    "    \"tcfd\",\n",
    "    \"sgr\",\n",
    "    \"integrated\",\n",
    "    \"integrado\",\n",
    "    \"annual\"\n",
    "]\n",
    "\n",
    "\"pdf file keywords filter\"\n",
    "search_terms = [ \"net zero\", \"social\", \"governance report\",'commitment']\n",
    "\n",
    "current_year = datetime.datetime.now().year\n",
    "numbers = [str(year)[-2:] for year in range(current_year-1, current_year+1)]\n",
    "regex_patterns = [re.compile(rf\"(?=.*{keyword})(?=.*{number})\", re.IGNORECASE) for number in numbers for keyword in patterns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def run_crawler(index, row):\n",
    "    \"\"\"\n",
    "    find whether the api returned url contains keywords and is created on 2023 \n",
    "    \"\"\"\n",
    "    keywords_url_list = []\n",
    "    pdf_keywords_url_dict = {}\n",
    "\n",
    "    for col in [\"result_1\", \"result_2\", \"result_3\"]:\n",
    "        url = row[col]\n",
    "        domain = row[\"domain\"]\n",
    "        actor_name = row[\"actor_name\"]\n",
    "        print(actor_name)\n",
    "        print(domain)\n",
    "\n",
    "        def task():\n",
    "            nonlocal keywords_url_list, pdf_keywords_url_dict\n",
    "            if isinstance(url, str) and ('.pdf' in url or check_pdf_url(url)):\n",
    "                if url not in keywords_url_list and filter_out_less2page(url):\n",
    "                    print(\"PDF more than 3 pages\")\n",
    "                    for regex_pattern in regex_patterns:\n",
    "                        if regex_pattern.search(url):\n",
    "                            keywords_url_list.append(url)\n",
    "                            print(\"URL contains keywords!\")\n",
    "                            break\n",
    "                    else:\n",
    "                        print(\"URL doesn't contain keywords, parsing the pdf....\") \n",
    "                        if check_pdf_file_for_keyword(url, [2023]) == True:\n",
    "                            print(\"PDF contains keywords!\")\n",
    "                            keywords_url_list.append(url)\n",
    "                    print(keywords_url_list)\n",
    "\n",
    "                    if len(list(set(keywords_url_list))) > 1:\n",
    "                        for pdf_url in keywords_url_list:\n",
    "                            new_dict = {domain: {pdf_url: get_pdf_create_date_page_num(pdf_url)}}\n",
    "                            for key, value in new_dict.items():\n",
    "                                if key in pdf_keywords_url_dict:\n",
    "                                    pdf_keywords_url_dict[key].update(value)\n",
    "                                else:\n",
    "                                    pdf_keywords_url_dict[key] = value\n",
    "                                print(pdf_keywords_url_dict)\n",
    "                                df_results.at[index, 'urls'] = find_max_url(df_results.at[index, 'domain'],pdf_keywords_url_dict)\n",
    "                                df_results.at[index, 'crawled'] = 'yes'\n",
    "\n",
    "                    else:\n",
    "                        try:\n",
    "                            df_results.at[index, 'urls'] = keywords_url_list[0]\n",
    "                        except:\n",
    "                            print(\"No PDF url created in 2023 and contains keywords....\")\n",
    "                        df_results.at[index, 'crawled'] = 'yes'\n",
    "                else:\n",
    "                    print(\"PDF less than 3 pages\")\n",
    "                \n",
    "        thread = threading.Thread(target=task)\n",
    "        thread.start()\n",
    "\n",
    "        # Wait for 180 seconds (3 minutes) for task to complete, otherwise assign 'Timeout, retry failed'\n",
    "        thread.join(60)\n",
    "        if thread.is_alive():\n",
    "            df_results.at[index, 'urls'] = 'Timeout, retry failed'\n",
    "            print(f\"Timeout, retry failed for index {index}\")\n",
    "            continue\n",
    "    try:\n",
    "        print(f\"Final picked url is {df_results.at[index, 'urls']}\")\n",
    "    except:\n",
    "        print(f\"No url pick\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"crapy data from google start\")\n",
    "\n",
    "df_results['urls'] = None\n",
    "\n",
    "crawl_deferreds = []\n",
    "for index, row in df_results.iterrows():\n",
    "    run_crawler(index, row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter Google returned pdf urls and store in the database\n",
    "con = sqlite3.connect('data2.db')\n",
    "df_results.to_sql(name=\"company\",con=con,index=False, if_exists=\"append\") # if need to append to original database\n",
    "# df_results.to_sql(name=\"company\",con=con,index=False, if_exists=\"replace\") # if need to replace with other database\n",
    "con.close()\n",
    "df_results.urls.values\n",
    "print(\"scrapying data from google pdf urls end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate spiders to crawl webpage urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first spider starts crawling google returned top3 urls; if not found, automatically second spider starts crawling from main site\n",
    "from run_spider import run_spider\n",
    "\n",
    "print(\"scrapying data from spider start\")\n",
    "run_spider()\n",
    "print(\"scrapying data from spider end\")\n",
    "\n",
    "print(\"success!! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('data2.db')\n",
    "# sql = \"select * from company \"\n",
    "print(\"check successful crawled urls...\")\n",
    "sql =  \"select * from company where not (urls is null or urls =='') and (urls != 'Timeout, retry failed')\"\n",
    "df = pd.read_sql(sql,con=con)\n",
    "df[['domain','urls','crawled']]\n",
    "df.urls.values\n",
    "df.shape\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare spider crawl urls with API urls and take most recent one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    print(f\"{row['actor_name']} is checking....\")\n",
    "    if (row['urls'] is not None) and (row['urls'] in [row['result_1'], row['result_2'], row['result_3']]):\n",
    "        row['final_urls'] = row['urls']\n",
    "    elif row['urls'] == 'Timeout, retry failed':\n",
    "        row['final_urls'] = row['urls']\n",
    "    else:\n",
    "        urls = [row['urls'], row['result_1'], row['result_2'], row['result_3']]\n",
    "        create_dates = []\n",
    "        page_counts = []\n",
    "        pdf_urls = []\n",
    "        for i, url in enumerate(urls):                \n",
    "            if isinstance(url, str) and (url is not None) and ('.pdf' in url or check_pdf_url(url)):\n",
    "                print(url)\n",
    "                for regex_pattern in regex_patterns:\n",
    "                    if regex_pattern.search(url):\n",
    "                        print(\"URL contains keywords!\")\n",
    "                        pdf_urls.append(url)\n",
    "                        create_date, page_count = get_pdf_create_date_page_num(url)\n",
    "                        create_dates.append(create_date)\n",
    "                        page_counts.append(page_count)\n",
    "                        break\n",
    "                else:\n",
    "                    print(\"URL doesn't contain keywords, parsing the pdf....\") \n",
    "                    if check_pdf_file_for_keyword(url, [2021, 2022, 2023]) == True:\n",
    "                        print(\"PDF contains keywords!\")\n",
    "                        pdf_urls.append(url)\n",
    "                        create_date, page_count = get_pdf_create_date_page_num(url)\n",
    "                        create_dates.append(create_date)\n",
    "                        page_counts.append(page_count)\n",
    "\n",
    "        print(create_dates)\n",
    "        print(page_counts)\n",
    "        try:\n",
    "            max_create_date = max(create_dates)\n",
    "            urls_with_max_create_date = [url for url, date in zip(pdf_urls, create_dates) if date == max_create_date]\n",
    "            print(urls_with_max_create_date)\n",
    "            max_page_count_url = max(urls_with_max_create_date, key=lambda url: page_counts[pdf_urls.index(url)])\n",
    "            print(f\"final url is {max_page_count_url}\")\n",
    "            row['final_urls'] = max_page_count_url\n",
    "\n",
    "        except:\n",
    "            print(\"There is no url meet our condition and let final url blank\")\n",
    "    \n",
    "    print(f\"{row['actor_name']} finished------\")\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start checking spider urls.....')\n",
    "df['final_urls'] = None\n",
    "\n",
    "df = df.apply(process_row, axis=1)\n",
    "print('Finish checking spider urls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.urls != df.final_urls][['final_urls','urls']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract txt and store txts in \"txt\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urls_to_txt import pdf_to_text\n",
    "\n",
    "print('Start extracting pdf to txt files.....')\n",
    "df = pdf_to_text(df, 'final_urls')\n",
    "\n",
    "# df.to_csv(\"company_pdf_txt.csv\", index=False)\n",
    "print(\"All pdf urls finished extraction!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_origin = pd.read_excel('Result_HERO_Companies w NZ Targets (06.06.23)_final.xlsx',sheet_name = 'Sheet1')\n",
    "\n",
    "df_merged = pd.merge(df_origin,df[['id_code','domain','final_urls','txt_name']], how='left', on = ['id_code'])  \n",
    "\n",
    "df_merged.to_csv(\"company_pdf_txt_merged.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
